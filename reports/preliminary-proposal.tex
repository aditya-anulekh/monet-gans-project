\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}



\usepackage[html,dvipsnames]{xcolor}


\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.0in}
\headheight=0.5in
\topmargin=-0.75in
\oddsidemargin= 0.0in
\evensidemargin=-0.25in


\usepackage[pdfauthor={Brandon Franzke},pdftitle={EE 599 Project Summary},% 
    pdftex,bookmarks]{hyperref}
\hypersetup{colorlinks,% 
    citecolor=green,%
    filecolor=Orange,%
    linkcolor=blue,%
    urlcolor=BrickRed,%
    pdftex}



\pagestyle{myheadings}
\markright{{\bf EE 541 - \copyright B. Franzke - Fall 2021} }


\title{\bf {\small EE 541 -- Computational Introduction to Deep Learning} \\ Initial Project Proposal}
\author{\copyright Prithvi Anandkumar Dalal, Shoumik Nandi, Aditya Anulekh Mantri}

\begin{document}
    \maketitle

    \paragraph{Project Title:}  Using GANs to create Monet Styled Paintings from Photos

    \paragraph{Project Team:} Prithvi Anandkumar Dalal, Shoumik Nandi, Aditya Anulekh Mantri

    \paragraph{Competition summary:}
    For this project we propose to create a model to create Monet-styled paintings from photos.
    Some of the applications of neural style transfer are:
    \begin{itemize}
        \item \textbf{Gaming and VR:} There are many cloud-powered video game streams that use image style transfer. CycleGANs assist developers to create interactive environments with personalised artistic styles to users.
        \item \textbf{Photo and Video Editing:} Style transfer can be used to edit and apply filters to photos and videos in real time.
        \item \textbf{Art and Entertainment:} Neural style transfer can be applied to art and paintings to recreate new ones.
    \end{itemize}


    \begin{figure}[!htb]
        \begin{center}
            \includegraphics[width=0.3\linewidth]{images/3b262c6726}
            \caption{The generated images should look similar to the Monet painting shown above}
            \label{fig:nseg}
        \end{center}
        \vspace{-0.6cm}
    \end{figure}

    \paragraph{Competition page:}
    \href
    {https://www.kaggle.com/c/gan-getting-started/}
    {https://www.kaggle.com/c/gan-getting-started/}

    \paragraph{Competition data:}
    The competition dataset is 385.87 MB.
    It consists of 300 Monet styled paintings and 7028 photos.
    Each image has 3 channels and is 256x256 pixels.
    We are supposed to submit 7000-10000 Monet styled images to the competition.

    Submissions are evaluated on MiFID (Memorization-informed Fréchet Inception Distance), which is a modification from Fréchet Inception Distance (FID).

    In FID, we use the Inception network to extract features from an intermediate layer.
    Then we model the data distribution for these features using a multivariate Gaussian distribution with mean $\mu$ and covariance $\Sigma$.
    The FID between the real images  and generated images  is computed as:

    \begin{displaymath}
        \text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr} (\Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{1/2})
    \end{displaymath}
    Where $Tr$ represents the trace of the matrix. More information about the evaluation of the competetion can be found here \href{https://www.kaggle.com/c/gan-getting-started/overview/evaluation}{here}

    \paragraph{Primary References and Codebase:}  We propose to build on the approach used in

    \begin{itemize}

        \item Zhu, Jun-Yan & Park, Taesung & Isola, Phillip & Efros, Alexei. (2017). "\href{https://arxiv.org/pdf/1703.10593.pdf}{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}" 2242-2251. 10.1109/ICCV.2017.244.

        \item Radford, Alec & Metz, Luke & Chintala, Soumith. (2016). "\href{https://arxiv.org/pdf/1511.06434.pdf}{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.}"

        \item Blog post: \href{https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#implementation}{DCGAN Tutorial}

        \item Github codebases: \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{CycleGAN and pix2pix in PyTorch},
    \end{itemize}


    \paragraph{Architecture Investigation Plan:}  We plan to first implement the CycleGAN architecture described in the paper by Jun-Yan Zhu et-al. We will then investigate the benefit of using a pre-trained model for generating Monet styled paintings, since the given dataset is very small and training a model from scratch might not be the most efficient approach.
    \paragraph{Estimated Compute Needs:}  Based on the data set size in the above paper and the benchmarks in this \href{https://lambdalabs.com/blog/titan-v-deep-learning-benchmarks/}{Lambda Labs Blog}, we estimate that one training run for our initial U-net architecture will take 14 hours on a single nVidia V100 GPU, which is the GPU resource in the AWS p3.2xlarge instance.  With spot pricing, which is roughly \$1 per hour, we expect \$15 per training run.  The proposed Mask R-CNN architecture will be roughly twice the complexity, so we estimation \$30 per run.  We expect to do a number of provisional runs to tune hyper-parameters.  We estimate that this will cost approximate \$40.  In addition, we expect to do approximately 4 full runs which brings our total estimated computing cost to roughly \$200.  Pooling our AWS promo resources, we expect to be about \$ 80 short of this value.

    \paragraph{Team Roles:} The following is the rough breakdown of roles and responsibilities we plan for our team:
    \begin{itemize}
        \item Prithvi Anandkumar Dalal: Data cleaning, augmentation and setup training pipeline on AWS
        \item Shoumik Nandi: Hyperparameter tuning and visualization
        \item Aditya Anulekh Mantri: Code and train CycleGAN model and assist with hyperparameter tuning
    \end{itemize}
    All team members will work on the final report and video.

% #TODO
%    Estimated Compute Needs
\end{document}
